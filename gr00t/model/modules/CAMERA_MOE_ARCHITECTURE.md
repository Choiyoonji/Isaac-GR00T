# Camera MoE (Mixture of Experts) ì•„í‚¤í…ì²˜ ë¬¸ì„œ

PI05 VLA ëª¨ë¸ì˜ ë©€í‹° ì¹´ë©”ë¼ ì…ë ¥ ì²˜ë¦¬ë¥¼ ìœ„í•œ MoE êµ¬í˜„ ì„¤ëª…ì„œ

## ğŸ“‹ ëª©ì°¨

1. [ì „ì²´ êµ¬ì¡° ê°œìš”](#ì „ì²´-êµ¬ì¡°-ê°œìš”)
2. [ë¼ìš°í„° (CameraRouter) êµ¬í˜„](#ë¼ìš°í„°-camerarouter-êµ¬í˜„)
3. [Camera MoE (CameraMoE) êµ¬í˜„](#camera-moe-cameramoe-êµ¬í˜„)
4. [Pi0 ëª¨ë¸ì—ì„œì˜ í†µí•©](#pi0-ëª¨ë¸ì—ì„œì˜-í†µí•©)
5. [ì„¤ê³„ ì² í•™](#ì„¤ê³„-ì² í•™)

---

## ì „ì²´ êµ¬ì¡° ê°œìš”

ì´ ì½”ë“œëŠ” ë©€í‹° ì¹´ë©”ë¼ ì…ë ¥ì„ ìœ„í•œ **ë¼ìš°í„° ê¸°ë°˜ ê²Œì´íŒ…(Router-based Gating)** ë°©ì‹ì˜ MoEë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ì „í†µì ì¸ expert networkê°€ ì•„ë‹ˆë¼, **routerê°€ ê° ì¹´ë©”ë¼ featureì— gating weightë¥¼ ì ìš©**í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

### ì£¼ìš” ì»´í¬ë„ŒíŠ¸

```
Input Cameras (cam1, cam2, cam3)
         â†“
    ViT Encoding
         â†“
    CameraRouter â†’ routing_weights [batch, 2]
         â†“
    Soft Gating (w âŠ™ F)
         â†“
   Feature Fusion
         â†“
    Output Tokens
```

### ì¹´ë©”ë¼ ë§¤í•‘

- **cam1**: L515/External camera (ì™¸ë¶€ ì¹´ë©”ë¼)
- **cam2**: Wrist camera (ì†ëª© ì¹´ë©”ë¼, base)
- **cam3**: Thermal/Right_Wrist camera (ì—´í™”ìƒ ì¹´ë©”ë¼)

---

## ë¼ìš°í„° (CameraRouter) êµ¬í˜„

**íŒŒì¼**: `camera_router.py`

### í•µì‹¬ ì„¤ê³„

ë¼ìš°í„°ëŠ” ë‹¤ì–‘í•œ ì…ë ¥ ì†ŒìŠ¤ë¡œë¶€í„° ì¹´ë©”ë¼ ì„ íƒ ê²°ì •ì„ í•™ìŠµí•©ë‹ˆë‹¤.

#### ì…ë ¥ íƒ€ì… (RouterInputType)

```python
class RouterInputType(str, Enum):
    PROMPT = "prompt"                    # cam2 + prompt
    STATE = "state"                      # cam2 + state
    PROMPT_STATE = "prompt_state"        # cam2 + prompt + state
```

### ì£¼ìš” êµ¬ì„±ìš”ì†Œ

#### 1. Learnable Attention Pooling

ë‹¨ìˆœ í‰ê·  í’€ë§ ëŒ€ì‹  í•™ìŠµ ê°€ëŠ¥í•œ ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ì—¬ promptì˜ ì¤‘ìš”í•œ ë‹¨ì–´ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.

```python
def _attention_pool(self, tokens):
    """
    Promptì—ì„œ ì¹´ë©”ë¼ ì„ íƒì— ì¤‘ìš”í•œ ë‹¨ì–´ë¥¼ í•™ìŠµ
    ì˜ˆ: "apple"ì´ë‚˜ "plate" ê°™ì€ í‚¤ì›Œë“œì— ì§‘ì¤‘
    """
    # Query í•™ìŠµ
    query = self.prompt_attention_query(...)
    
    # Attention ê³„ì‚°: QÂ·K^T / sqrt(d)
    attention_scores = jnp.sum(query * tokens, axis=-1) / scale
    attention_weights = softmax(attention_scores)
    
    # Weighted sum
    pooled = jnp.sum(tokens * attention_weights[:, :, None], axis=1)
    return pooled
```

**ì¥ì **:
- íƒœìŠ¤í¬ì™€ ê´€ë ¨ëœ prompt í‚¤ì›Œë“œì— ì§‘ì¤‘
- ë‹¨ìˆœ í‰ê·  í’€ë§ë³´ë‹¤ ì •ë³´ ì†ì‹¤ ê°ì†Œ

#### 2. Router MLP êµ¬ì¡°

```
Input Features â†’ Linear1(512) â†’ ReLU â†’ Linear2(256) â†’ ReLU â†’ Linear3(2) â†’ Logits
```

**ì…ë ¥ ì°¨ì› ê³„ì‚°**:
- `PROMPT`: `embed_dim + embed_dim` (cam2 + prompt)
- `STATE`: `embed_dim + state_dim` (cam2 + state)
- `PROMPT_STATE`: `embed_dim + embed_dim + state_dim` (cam2 + prompt + state)

#### 3. Routing Weight ê³„ì‚°

```python
def __call__(self, cam1_tokens, cam2_tokens, prompt_tokens, state, train, rng):
    # 1. ì„¤ì •ëœ input_typeì— ë”°ë¼ features ìˆ˜ì§‘
    features = []
    if needs_cam2:
        cam2_pooled = jnp.mean(cam2_tokens, axis=1)
        features.append(cam2_pooled)
    if needs_prompt:
        prompt_pooled = self._attention_pool(prompt_tokens)  # Learnable pooling
        features.append(prompt_pooled)
    if needs_state:
        features.append(state)
    
    # 2. Feature concatenation
    combined_features = jnp.concatenate(features, axis=-1)
    
    # 3. MLPë¥¼ í†µí•œ logits ê³„ì‚°
    x = nnx.relu(self.router_linear1(combined_features))
    x = nnx.relu(self.router_linear2(x))
    logits = self.router_linear3(x)  # [batch, num_experts=2]
    
    # 4. Temperature scaling
    logits = logits / self.temperature
    
    # 5. Softmax ë˜ëŠ” Gumbel-Softmax
    if train and self.use_gumbel:
        routing_weights = self._gumbel_softmax(logits, rng, temperature=self.gumbel_temp)
    else:
        routing_weights = jax.nn.softmax(logits, axis=-1)
    
    return routing_weights  # [batch, 2] where [:, 0]=L515, [:, 1]=Thermal
```

#### 4. Focal Loss for Routing

Hard exampleì— ì§‘ì¤‘í•˜ëŠ” Focal Lossë¥¼ ì‚¬ìš©í•˜ì—¬ routerë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

```python
def compute_routing_loss(self, routing_weights, target_camera_idx, alpha=0.25, gamma=2.0):
    """
    Focal Loss = -Î± * (1 - p_t)^Î³ * log(p_t)
    
    Args:
        routing_weights: [batch, 2] ì˜ˆì¸¡ëœ ì¹´ë©”ë¼ gating weights
        target_camera_idx: [batch] ground truth (0=L515, 1=Thermal)
        alpha: 0.25 (positive/negative example ê· í˜•)
        gamma: 2.0 (focusing parameter, í´ìˆ˜ë¡ hard exampleì— ì§‘ì¤‘)
    """
    # Target camera probability
    p_t = jnp.take_along_axis(routing_weights, target_camera_idx[:, None], axis=1)
    p_t = jnp.clip(p_t, 1e-10, 1.0 - 1e-10)
    
    # Focal weight: (1 - p_t)^gamma
    focal_weight = jnp.power(1.0 - p_t, gamma)
    
    # Focal Loss
    loss = -alpha * focal_weight * jnp.log(p_t)
    return jnp.mean(loss)
```

**Focal Lossì˜ ì¥ì **:
- Well-classified exampleì€ down-weight (ì‰¬ìš´ ì˜ˆì œ ë¬´ì‹œ)
- Misclassified exampleì— ì§‘ì¤‘ (ì–´ë ¤ìš´ ì˜ˆì œ ê°•ì¡°)
- `gamma=0`ì´ë©´ standard cross-entropyë¡œ í™˜ì›

---

## Camera MoE (CameraMoE) êµ¬í˜„

**íŒŒì¼**: `camera_router.py`

### í•µì‹¬: Soft Gating with Learnable Scaling

ì „í†µì ì¸ expert networkë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , **ì§ì ‘ feature gating**ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### ìˆ˜í•™ì  ì •ì˜

```
F_gated = Î³ âŠ™ (w âŠ™ F_original)
```

**ë³€ìˆ˜ ì„¤ëª…**:
- `F_original`: ViTì—ì„œ ë‚˜ì˜¨ ì›ë³¸ ì¹´ë©”ë¼ features `[batch, seq_len, embed_dim]`
- `w`: Routerê°€ ì˜ˆì¸¡í•œ gating weight (ìŠ¤ì¹¼ë¼, 0-1 ë²”ìœ„)
- `Î³`: í•™ìŠµ ê°€ëŠ¥í•œ scale parameter (ì±„ë„ë³„ ë²¡í„° `[embed_dim]`)
- `âŠ™`: Element-wise multiplication

### êµ¬í˜„ ë‹¨ê³„

#### Step 1: Router í˜¸ì¶œ

```python
routing_weights = self.router(
    cam1_tokens=cam1_tokens,      # L515 features [batch, seq_len, embed_dim]
    cam2_tokens=cam2_tokens,      # Wrist features (base)
    prompt_tokens=prompt_tokens,  # Prompt embeddings
    state=state,                  # Robot state vector
    train=train,
    rng=rng,
)
# ì¶œë ¥: [batch, 2] where [:, 0]=w_L515, [:, 1]=w_Thermal
```

#### Step 2: Soft Gating ì ìš©

```python
# Routing weights ì¶”ì¶œ
w_cam1 = routing_weights[:, 0:1, None]  # [batch, 1, 1]
w_cam3 = routing_weights[:, 1:2, None]  # [batch, 1, 1]

if self.use_learnable_scales:
    # Two-step gating with learnable scaling
    
    # Step 1: Router gating (w âŠ™ F)
    cam1_weighted = cam1_tokens * w_cam1  # [batch, seq_len, embed_dim]
    cam3_weighted = cam3_tokens * w_cam3
    
    # Step 2: Learnable magnitude compensation (Î³ âŠ™ (w âŠ™ F))
    cam1_gated = cam1_weighted * self.scale_cam1.value  # [batch, seq_len, embed_dim]
    cam3_gated = cam3_weighted * self.scale_cam3.value
else:
    # ë‹¨ìˆœ gating (êµ¬ë²„ì „ í˜¸í™˜ìš©)
    cam1_gated = cam1_tokens * w_cam1
    cam3_gated = cam3_tokens * w_cam3
```

**Learnable Scalingì˜ ì—­í• **:
- Gatingìœ¼ë¡œ ì¸í•œ magnitude ê°ì†Œë¥¼ ë³´ìƒ
- ê° ì±„ë„ë³„ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ scale ì ìš©
- ì´ˆê¸°ê°’ì€ 1.0 (identity transformation)

#### Step 3: Feature Fusion

```python
# ëª¨ë“  ì¹´ë©”ë¼ feature ê²°í•©
concatenated = jnp.concatenate([
    cam2_tokens,   # Wrist (base, always included)
    cam1_gated,    # L515 (gated)
    cam3_gated     # Thermal (gated)
], axis=-1)
# Shape: [batch, seq_len, 3 * embed_dim]

# Projection layerë¡œ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³µì›
output_tokens = self.projection(concatenated)
# Shape: [batch, seq_len, embed_dim]

return output_tokens, routing_weights
```

### ì „ì²´ Forward Pass

```python
def __call__(
    self,
    cam2_tokens,      # Wrist camera (base) [batch, seq_len, embed_dim]
    cam1_tokens,      # L515/External camera [batch, seq_len, embed_dim]
    cam3_tokens,      # Thermal camera [batch, seq_len, embed_dim]
    prompt_tokens,    # Prompt [batch, prompt_len, embed_dim]
    state,            # Robot state [batch, state_dim]
    train,
    rng,
) -> tuple[output_tokens, routing_weights]:
    """
    Returns:
        output_tokens: Fused camera features [batch, seq_len, embed_dim]
        routing_weights: Camera gating weights [batch, 2]
    """
    # 1. Router: ì¹´ë©”ë¼ë³„ gating weight ê³„ì‚°
    routing_weights = self.router(...)
    
    # 2. Missing camera ì²˜ë¦¬ (zero padding)
    if cam1_tokens is None: cam1_tokens = jnp.zeros_like(cam2_tokens)
    if cam3_tokens is None: cam3_tokens = jnp.zeros_like(cam2_tokens)
    
    # 3. Soft gating ì ìš©
    w_cam1 = routing_weights[:, 0:1, None]
    w_cam3 = routing_weights[:, 1:2, None]
    cam1_gated = cam1_tokens * w_cam1 * self.scale_cam1
    cam3_gated = cam3_tokens * w_cam3 * self.scale_cam3
    
    # 4. Feature fusion
    concatenated = jnp.concatenate([cam2_tokens, cam1_gated, cam3_gated], axis=-1)
    output_tokens = self.projection(concatenated)
    
    return output_tokens, routing_weights
```

### Routing Loss ê³„ì‚°

```python
def compute_routing_loss(self, routing_weights, cam1_activate, cam3_activate):
    """
    Ground truth labelsë¥¼ ê¸°ë°˜ìœ¼ë¡œ routing loss ê³„ì‚°
    
    Args:
        routing_weights: [batch, 2] ì˜ˆì¸¡ëœ gating weights
        cam1_activate: [batch] binary (1=L515 í™œì„±í™”)
        cam3_activate: [batch] binary (1=Thermal í™œì„±í™”)
    """
    # Labelì„ target camera indexë¡œ ë³€í™˜
    target_camera_idx = jnp.where(
        cam1_activate == 1,
        0,  # L515
        jnp.where(cam3_activate == 1, 1, 0)  # Thermal or default L515
    )
    
    return self.router.compute_routing_loss(routing_weights, target_camera_idx)
```

---

## Pi0 ëª¨ë¸ì—ì„œì˜ í†µí•©

**íŒŒì¼**: `pi0.py`

### ì´ˆê¸°í™”

```python
class Pi0(_model.BaseModel):
    def __init__(self, config: pi0_config.Pi0Config, rngs: nnx.Rngs):
        # ... PaliGemma, ViT ì´ˆê¸°í™” ...
        
        # Camera MoE ì´ˆê¸°í™”
        if config.use_camera_moe:
            from openpi.models.camera_router import CameraMoE, CameraRouterConfig
            
            router_config = CameraRouterConfig(
                embed_dim=paligemma_config.width,  # 2048
                state_dim=config.state_dim,         # 32
                router_input_type=config.camera_router_input_type,  # "prompt", "state", etc.
                use_attention_pooling=config.camera_router_use_attention_pooling,
                use_learnable_scales=config.camera_router_use_learnable_scales,
                num_experts=2,  # cam2 vs cam3 ì„ íƒ
                router_hidden_dim=config.camera_router_hidden_dim,
                router_temperature=config.camera_router_temperature,
                use_gumbel_softmax=config.camera_router_use_gumbel,
                gumbel_temperature=config.camera_router_gumbel_temp,
            )
            self.camera_moe = CameraMoE(router_config, rngs)
            self.routing_loss_weight = config.camera_routing_loss_weight
```

### ì‚¬ìš© ì˜ˆì‹œ

```python
def embed_prefix(self, obs, rng, router_weights=None):
    """Prefix embedding with Camera MoE"""
    
    if self.use_camera_moe and self.camera_moe is not None:
        # 1. ê° ì¹´ë©”ë¼ë³„ë¡œ ViT encoding
        cam1_tokens = None
        cam2_tokens = None
        cam3_tokens = None
        
        for name in obs.images:
            image_tokens, _ = self.PaliGemma.img(obs.images[name], train=False)
            
            # ì¹´ë©”ë¼ ì´ë¦„ì—ì„œ cam1/cam2/cam3 ë§¤í•‘
            if "base" in name or name == "image":
                cam1_tokens = image_tokens
            elif "wrist" in name and "right" not in name:
                cam2_tokens = image_tokens
            elif "right_wrist" in name or "thermal" in name:
                cam3_tokens = image_tokens
        
        # 2. Prompt embedding
        prompt_tokens = self.PaliGemma.llm(obs.tokenized_prompt, method="embed")
        
        # 3. Camera MoEë¥¼ í†µí•œ multi-camera fusion
        fused_tokens, routing_weights = self.camera_moe(
            cam2_tokens,          # Base camera (wrist)
            cam1_tokens,          # L515 (external)
            cam3_tokens,          # Thermal (right wrist)
            prompt_tokens=prompt_tokens,
            state=obs.state,      # Robot state
            train=not self.deterministic,
            rng=rng,
        )
        
        # 4. Fused tokensë¥¼ token listì— ì¶”ê°€
        tokens.append(fused_tokens)
        tokens.append(prompt_tokens)
        
        return tokens, routing_weights
```

### Configuration ì˜ˆì‹œ

```python
# pi0_config.py
@dataclasses.dataclass(frozen=True)
class Pi0Config(_model.BaseModelConfig):
    # Camera Router/MoE configuration
    use_camera_moe: bool = False
    camera_router_hidden_dim: int = 512
    camera_router_temperature: float = 1.0
    camera_router_use_gumbel: bool = False
    camera_router_gumbel_temp: float = 1.0
    camera_routing_loss_weight: float = 0.1
    camera_router_input_type: str = "prompt"  # "prompt", "state", "prompt_state"
    camera_router_use_attention_pooling: bool = False
    camera_router_use_learnable_scales: bool = False
```

---

## ì„¤ê³„ ì² í•™

### 1. Expert Networkë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì´ìœ 

**ì „í†µì ì¸ MoE**:
```
Router â†’ Expert1, Expert2, ... â†’ Weighted combination
```

**ë³¸ êµ¬í˜„ (Direct Gating)**:
```
Router â†’ Gating weights â†’ Direct feature scaling
```

**ì´ìœ **:
- ViT featuresê°€ ì´ë¯¸ ì¶©ë¶„íˆ semanticí•¨
- Expert network ì¶”ê°€ëŠ” ë¶ˆí•„ìš”í•œ ë³µì¡ë„ ì¦ê°€
- ì§ì ‘ gatingì´ ë” íš¨ìœ¨ì ì´ê³  í•´ì„ ê°€ëŠ¥í•¨

### 2. Soft Gatingì˜ ì¥ì 

**Hard Gating (discrete selection)**:
```
if w_L515 > w_Thermal:
    output = cam1_features  # L515ë§Œ ì‚¬ìš©
else:
    output = cam3_features  # Thermalë§Œ ì‚¬ìš©
```

**Soft Gating (continuous weighting)**:
```
output = 0.7 * cam1_features + 0.3 * cam3_features
```

**ì¥ì **:
1. **Train-test ì¼ê´€ì„±**: Trainingê³¼ inferenceì—ì„œ ë™ì¼í•œ ë™ì‘
2. **ë¶€ë“œëŸ¬ìš´ fusion**: ì¹´ë©”ë¼ ê°„ smooth transition
3. **ì •ë³´ ë³´ì¡´**: ëª¨ë“  ì¹´ë©”ë¼ ì •ë³´ë¥¼ ì¼ë¶€ë¼ë„ í™œìš©
4. **Gradient flow**: ëª¨ë“  ì¹´ë©”ë¼ì— ëŒ€í•´ gradient ì „íŒŒ ê°€ëŠ¥

### 3. Learnable Scalingì˜ í•„ìš”ì„±

**ë¬¸ì œ**: Gatingìœ¼ë¡œ ì¸í•œ magnitude ê°ì†Œ

```python
# w < 1.0ì´ë©´ feature magnitude ê°ì†Œ
F_gated = w * F_original  # magnitudeê°€ ì¤„ì–´ë“¦
```

**í•´ê²°**: Learnable scale parameter

```python
# Î³ë¡œ magnitude ë³´ìƒ
F_gated = Î³ * w * F_original
# Î³ëŠ” í•™ìŠµì„ í†µí•´ ìµœì  scale ì°¾ìŒ
```

**íš¨ê³¼**:
- Feature magnitude ìœ ì§€
- ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ layerì— ì•ˆì •ì ì¸ ì…ë ¥ ì œê³µ
- ì„±ëŠ¥ í–¥ìƒ

### 4. Focal Loss ì‚¬ìš© ì´ìœ 

**Standard Cross-Entropy**:
```
Loss = -log(p_t)
# ëª¨ë“  exampleì„ ë™ë“±í•˜ê²Œ ì²˜ë¦¬
```

**Focal Loss**:
```
Loss = -(1 - p_t)^Î³ * log(p_t)
# Hard exampleì— ì§‘ì¤‘
```

**íš¨ê³¼**:
- Well-classified example (p_t â‰ˆ 1) â†’ Loss â‰ˆ 0
- Misclassified example (p_t â‰ˆ 0) â†’ Loss í¬ê²Œ ì¦ê°€
- Routerê°€ ì–´ë ¤ìš´ case í•™ìŠµì— ì§‘ì¤‘

### 5. ì¹´ë©”ë¼ ì„ íƒ ì „ëµ

**Base Camera (cam2 - Wrist)**:
- í•­ìƒ í¬í•¨ (gating ì—†ìŒ)
- ì†ëª© ìœ„ì¹˜ì—ì„œì˜ ê´€ì  ì œê³µ
- ë¡œë´‡ì˜ end-effector ê·¼ì²˜ ì •ë³´

**Auxiliary Cameras (cam1, cam3)**:
- Routerê°€ ì„ íƒì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
- cam1 (L515): ì™¸ë¶€ ê´€ì , ë„“ì€ ì‹œì•¼
- cam3 (Thermal): ì˜¨ë„ ì •ë³´, íŠ¹ìˆ˜ ì„¼ì„œ

**ì„ íƒ ê¸°ì¤€**:
- Prompt ë‚´ìš© (ì˜ˆ: "hot", "cold" â†’ Thermal ì„ í˜¸)
- Robot state (íŠ¹ì • ìì„¸ì—ì„œ íŠ¹ì • ì¹´ë©”ë¼ ìœ ìš©)
- Task context

---

## ì‚¬ìš© ì˜ˆì‹œ

### Training

```python
# Forward pass
fused_tokens, routing_weights = model.camera_moe(
    cam2_tokens, cam1_tokens, cam3_tokens,
    prompt_tokens=prompt_tokens,
    state=robot_state,
    train=True,
    rng=rng_key,
)

# Compute losses
action_loss = compute_action_loss(...)

# Routing loss (ground truth camera labels í•„ìš”)
routing_loss = model.camera_moe.compute_routing_loss(
    routing_weights,
    cam1_activate=ground_truth_cam1,  # [batch] binary
    cam3_activate=ground_truth_cam3,  # [batch] binary
)

# Total loss
total_loss = action_loss + 0.1 * routing_loss
```

### Inference

```python
# Routerê°€ ìë™ìœ¼ë¡œ ì¹´ë©”ë¼ ì„ íƒ
fused_tokens, routing_weights = model.camera_moe(
    cam2_tokens, cam1_tokens, cam3_tokens,
    prompt_tokens=prompt_tokens,
    state=robot_state,
    train=False,
)

# routing_weights í™•ì¸
print(f"L515 weight: {routing_weights[0, 0]:.3f}")
print(f"Thermal weight: {routing_weights[0, 1]:.3f}")
```

---

## íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

### Router Configuration

```python
CameraRouterConfig(
    embed_dim=2048,              # PaliGemmaì˜ embedding dimension
    state_dim=32,                # Robot state vector size
    router_input_type="prompt",  # Routerê°€ ì‚¬ìš©í•  ì…ë ¥ ì„ íƒ
    use_attention_pooling=True,  # Prompt attention pooling ì‚¬ìš© (ê¶Œì¥)
    use_learnable_scales=True,   # Learnable scaling ì‚¬ìš© (ê¶Œì¥)
    num_experts=2,               # ì¹´ë©”ë¼ ê°œìˆ˜
    router_hidden_dim=512,       # Router MLP hidden dimension
    router_temperature=1.0,      # Softmax temperature (ë‚®ì„ìˆ˜ë¡ discrete)
    use_gumbel_softmax=False,    # Gumbel-Softmax ì‚¬ìš© ì—¬ë¶€
    gumbel_temperature=1.0,      # Gumbel temperature
)
```

**íŠœë‹ íŒ**:
- `router_temperature`: 0.5-2.0 ë²”ìœ„ì—ì„œ ì‹¤í—˜
  - ë‚®ìŒ â†’ ë” discreteí•œ ì„ íƒ
  - ë†’ìŒ â†’ ë” smoothí•œ blending
- `router_hidden_dim`: 256-1024
  - ì‘ìŒ â†’ ë¹ ë¥´ì§€ë§Œ í‘œí˜„ë ¥ ë‚®ìŒ
  - í¼ â†’ ëŠë¦¬ì§€ë§Œ í‘œí˜„ë ¥ ë†’ìŒ
- `use_attention_pooling=True`: ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì„±ëŠ¥ í–¥ìƒ
- `use_learnable_scales=True`: ì„±ëŠ¥ í–¥ìƒì— í•„ìˆ˜

### Training Hyperparameters

```python
camera_routing_loss_weight=0.1  # Routing loss ê°€ì¤‘ì¹˜ (0.05-0.2 ë²”ìœ„)
```

**ì¡°ì • ê¸°ì¤€**:
- ë„ˆë¬´ ë‚®ìŒ â†’ Routerê°€ í•™ìŠµë˜ì§€ ì•ŠìŒ
- ë„ˆë¬´ ë†’ìŒ â†’ Action prediction ì„±ëŠ¥ ì €í•˜
- ì‹œì‘: 0.1, í•„ìš”ì‹œ ì¡°ì •

---

## ì°¸ê³  ìë£Œ

- **Original MoE Paper**: Shazeer et al., "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
- **Focal Loss**: Lin et al., "Focal Loss for Dense Object Detection"
- **Gumbel-Softmax**: Jang et al., "Categorical Reparameterization with Gumbel-Softmax"
- **PaliGemma**: Google Research, "PaliGemma: A versatile 3B VLM for transfer"

---

## FAQ

### Q1: ì™œ 3ê°œ ì¹´ë©”ë¼ë¥¼ ëª¨ë‘ fusioní•˜ë‚˜ìš”?

**A**: cam2 (wrist)ë¥¼ baseë¡œ ì‚¬ìš©í•˜ê³ , cam1 (L515)ê³¼ cam3 (Thermal)ì„ ì„ íƒì ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´:
- Wrist ê´€ì ì€ í•­ìƒ ìœ ì§€
- ì¶”ê°€ ì •ë³´ëŠ” í•„ìš”í•  ë•Œë§Œ í™œìš©
- ë¶ˆí•„ìš”í•œ ì •ë³´ë¡œ ì¸í•œ noise ê°ì†Œ

### Q2: Soft gating vs Hard gating ì¤‘ ì–´ëŠ ê²ƒì´ ì¢‹ë‚˜ìš”?

**A**: **Soft gating ê¶Œì¥**
- Train-test consistency ë³´ì¥
- ëª¨ë“  ì¹´ë©”ë¼ ì •ë³´ í™œìš© ê°€ëŠ¥
- Gradientê°€ ëª¨ë“  ì¹´ë©”ë¼ì— íë¦„
- Productionì—ì„œ ë” ì•ˆì •ì 

### Q3: Expert network ì—†ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?

**A**: ViT featuresê°€ ì´ë¯¸ ì¶©ë¶„íˆ semanticí•˜ë¯€ë¡œ:
- Direct gatingë§Œìœ¼ë¡œ íš¨ê³¼ì 
- Expert networkëŠ” ë¶ˆí•„ìš”í•œ ë³µì¡ë„
- ë” ë¹ ë¥´ê³  í•´ì„ ê°€ëŠ¥

### Q4: Learnable scalingì´ ì •ë§ í•„ìš”í•œê°€ìš”?

**A**: **ë„¤, ê¶Œì¥í•©ë‹ˆë‹¤**
- Gatingìœ¼ë¡œ ì¸í•œ magnitude ê°ì†Œ ë³´ìƒ
- ì‹¤í—˜ ê²°ê³¼ ì„±ëŠ¥ í–¥ìƒ í™•ì¸
- ê³„ì‚° ë¹„ìš©ì€ ë¯¸ë¯¸í•¨ (channel-wise multiplication)

### Q5: Router input typeì„ ì–´ë–»ê²Œ ì„ íƒí•˜ë‚˜ìš”?

**A**:
- `"prompt"`: Task descriptionì´ ì¹´ë©”ë¼ ì„ íƒì— ì¤‘ìš”í•œ ê²½ìš°
- `"state"`: Robot ìì„¸/ìœ„ì¹˜ê°€ ì¹´ë©”ë¼ ì„ íƒì— ì¤‘ìš”í•œ ê²½ìš°
- `"prompt_state"`: ë‘˜ ë‹¤ ì¤‘ìš”í•œ ê²½ìš° (ê¶Œì¥)

---

## ë²„ì „ í˜¸í™˜ì„±

### Backward Compatibility

êµ¬ë²„ì „ checkpoint ë¡œë”©ì„ ìœ„í•œ ì˜µì…˜:

```python
# ì˜µì…˜ 1: Attention pooling ì—†ì´ (êµ¬ë²„ì „)
use_attention_pooling=False  # Mean pooling ì‚¬ìš©

# ì˜µì…˜ 2: Learnable scales ì—†ì´ (êµ¬ë²„ì „)
use_learnable_scales=False   # Simple gatingë§Œ ì‚¬ìš©
```

### Migration Guide

êµ¬ë²„ì „ì—ì„œ ì‹ ë²„ì „ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜:

```python
# 1. ë¨¼ì € ê¸°ì¡´ ì„¤ì •ìœ¼ë¡œ ë¡œë“œ
config = Pi0Config(
    use_camera_moe=True,
    camera_router_use_attention_pooling=False,
    camera_router_use_learnable_scales=False,
)

# 2. Checkpoint ë¡œë“œ
model = config.create(rng_key)
model = load_checkpoint(model, old_checkpoint_path)

# 3. ìƒˆë¡œìš´ ê¸°ëŠ¥ í™œì„±í™” í›„ fine-tuning
config = Pi0Config(
    use_camera_moe=True,
    camera_router_use_attention_pooling=True,   # âœ“
    camera_router_use_learnable_scales=True,    # âœ“
)
model = config.create(rng_key)
# ì´ì „ weight ë¡œë“œ í›„ ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ
```

---

**Last Updated**: January 14, 2026  
**Maintained by**: OpenPI Team
