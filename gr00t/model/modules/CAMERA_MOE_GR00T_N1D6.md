# Camera MoE Integration for GR00T N1d6

PI05 VLA ëª¨ë¸ì˜ ë©€í‹° ì¹´ë©”ë¼ ë¼ìš°í„°ë¥¼ GR00T N1d6ì— ì ìš©í•œ êµ¬í˜„ ë¬¸ì„œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [ì¹´ë©”ë¼ êµ¬ì„±](#ì¹´ë©”ë¼-êµ¬ì„±)
3. [ì•„í‚¤í…ì²˜](#ì•„í‚¤í…ì²˜)
4. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
5. [ì„¤ì • ì˜µì…˜](#ì„¤ì •-ì˜µì…˜)
6. [ë°ì´í„° í˜•ì‹](#ë°ì´í„°-í˜•ì‹)
7. [ì˜ˆì œ](#ì˜ˆì œ)

---

## ê°œìš”

GR00T N1d6 ëª¨ë¸ì— Camera MoE (Mixture of Experts)ë¥¼ í†µí•©í•˜ì—¬ ë©€í‹° ì¹´ë©”ë¼ ì…ë ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

- **Router ê¸°ë°˜ Gating**: ê° ì¹´ë©”ë¼ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•˜ì—¬ ì ì‘ì  fusion
- **Soft Gating**: ëª¨ë“  ì¹´ë©”ë¼ ì •ë³´ë¥¼ ë¶€ë“œëŸ½ê²Œ ê²°í•©
- **Learnable Scaling**: Gatingìœ¼ë¡œ ì¸í•œ magnitude ê°ì†Œë¥¼ ë³´ìƒ
- **Focal Loss**: Hard exampleì— ì§‘ì¤‘í•˜ëŠ” routing loss

---

## ì¹´ë©”ë¼ êµ¬ì„±

GR00T N1d6ì˜ ì¹´ë©”ë¼ ì„¤ì •:

```
cam1: L515/External camera (ì™¸ë¶€ ì¹´ë©”ë¼, base - í•­ìƒ í¬í•¨)
cam2: Left Wrist camera (ì™¼ì† ì¹´ë©”ë¼, gated)
cam3: Right Wrist camera (ì˜¤ë¥¸ì† ì¹´ë©”ë¼, gated)
```

### PI05ì™€ì˜ ì°¨ì´ì 

| í•­ëª© | PI05 | GR00T N1d6 |
|------|------|------------|
| Base Camera | cam2 (Wrist) | cam1 (L515/External) |
| Auxiliary Camera 1 | cam1 (L515) | cam2 (Left Wrist) |
| Auxiliary Camera 2 | cam3 (Thermal) | cam3 (Right Wrist) |
| Router Input | cam2 + prompt + state | cam1 + prompt + state |

---

## ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡°

```
Input: cam1, cam2, cam3 (multi-camera images)
         â†“
    Eagle Backbone (ê° ì¹´ë©”ë¼ë³„ë¡œ ì²˜ë¦¬)
         â†“
    Camera Features: [batch, seq_len, embed_dim]
         â†“
    CameraRouter(cam1_features, prompt, state) â†’ routing_weights [batch, 2]
         â†“
    Soft Gating: w âŠ™ F
         â†“
    Learnable Scaling: Î³ âŠ™ (w âŠ™ F)
         â†“
    Feature Fusion: concat(cam1, cam2_gated, cam3_gated) â†’ Projection
         â†“
    Fused Features â†’ Action Head
```

### ìˆ˜í•™ì  ì •ì˜

**Soft Gating with Learnable Scaling**:
```
F_gated = Î³ âŠ™ (w âŠ™ F_original)
```

- `F_original`: Eagle backboneì—ì„œ ë‚˜ì˜¨ ì¹´ë©”ë¼ features
- `w`: Routerê°€ ì˜ˆì¸¡í•œ gating weight (0-1 ë²”ìœ„ì˜ ìŠ¤ì¹¼ë¼)
- `Î³`: í•™ìŠµ ê°€ëŠ¥í•œ scale parameter (ì±„ë„ë³„ ë²¡í„°)

**Routing Loss (Focal Loss)**:
```
Loss = -Î± * (1 - p_t)^Î³ * log(p_t)
```

- `p_t`: ì •ë‹µ ì¹´ë©”ë¼ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ 
- `Î±`: 0.25 (positive/negative example ê· í˜•)
- `Î³`: 2.0 (focusing parameter)

---

## ì‚¬ìš© ë°©ë²•

### 1. Config ì„¤ì •

`gr00t/configs/model/gr00t_n1d6.py` ë˜ëŠ” YAML config:

```python
from gr00t.configs.model.gr00t_n1d6 import Gr00tN1d6Config

config = Gr00tN1d6Config(
    # ê¸°ì¡´ ì„¤ì •...
    model_name="nvidia/Eagle-Block2A-2B-v2",
    
    # Camera MoE ì„¤ì •
    use_camera_moe=True,  # Camera MoE í™œì„±í™”
    camera_router_hidden_dim=512,  # Router MLP hidden dimension
    camera_router_temperature=1.0,  # Softmax temperature
    camera_router_use_gumbel=False,  # Gumbel-Softmax ì‚¬ìš© ì—¬ë¶€
    camera_router_gumbel_temp=1.0,  # Gumbel temperature
    camera_routing_loss_weight=0.1,  # Routing loss ê°€ì¤‘ì¹˜
    camera_router_use_attention_pooling=True,  # Attention pooling ì‚¬ìš© (ê¶Œì¥)
    camera_router_use_learnable_scales=True,  # Learnable scaling ì‚¬ìš© (ê¶Œì¥)
)
```

### 2. ëª¨ë¸ ì´ˆê¸°í™”

```python
from gr00t.model.gr00t_n1d6.gr00t_n1d6 import Gr00tN1d6

model = Gr00tN1d6(config)

# Camera MoEê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
if model.use_camera_moe:
    print(f"Camera MoE enabled: {model.camera_moe}")
else:
    print("Camera MoE disabled - using single camera mode")
```

### 3. Training

```python
# Multi-camera ì…ë ¥ ì¤€ë¹„
inputs = {
    # Camera 1 (Base - L515/External)
    "cam1_pixel_values": cam1_images,  # [batch, channels, height, width]
    "cam1_input_ids": cam1_text_ids,
    "cam1_attention_mask": cam1_attn_mask,
    
    # Camera 2 (Left Wrist)
    "cam2_pixel_values": cam2_images,
    "cam2_input_ids": cam2_text_ids,
    "cam2_attention_mask": cam2_attn_mask,
    
    # Camera 3 (Right Wrist)
    "cam3_pixel_values": cam3_images,
    "cam3_input_ids": cam3_text_ids,
    "cam3_attention_mask": cam3_attn_mask,
    
    # Action inputs
    "state": robot_state,  # [batch, state_dim]
    "action": actions,  # [batch, action_horizon, action_dim]
    "embodiment_id": embodiment_ids,  # [batch]
    "action_mask": action_mask,  # [batch, action_horizon, action_dim]
    
    # Ground truth camera labels (routing lossë¥¼ ìœ„í•´)
    "cam2_activate": cam2_labels,  # [batch] binary (1=cam2 should be active)
    "cam3_activate": cam3_labels,  # [batch] binary (1=cam3 should be active)
}

# Forward pass
outputs = model(inputs)

# Loss í™•ì¸
total_loss = outputs["loss"]  # action_loss + routing_loss_weight * routing_loss
action_loss = outputs.get("action_loss")
routing_loss = outputs.get("routing_loss")
routing_weights = outputs.get("routing_weights")  # [batch, 2]

print(f"Total Loss: {total_loss.item():.4f}")
print(f"Action Loss: {action_loss.mean().item():.4f}")
if routing_loss is not None:
    print(f"Routing Loss: {routing_loss.item():.4f}")
if routing_weights is not None:
    print(f"Routing Weights (cam2, cam3): {routing_weights[0]}")
```

### 4. Inference

```python
# Multi-camera ì…ë ¥ (ground truth labels ë¶ˆí•„ìš”)
inputs = {
    "cam1_pixel_values": cam1_images,
    "cam1_input_ids": cam1_text_ids,
    "cam1_attention_mask": cam1_attn_mask,
    
    "cam2_pixel_values": cam2_images,
    "cam2_input_ids": cam2_text_ids,
    "cam2_attention_mask": cam2_attn_mask,
    
    "cam3_pixel_values": cam3_images,
    "cam3_input_ids": cam3_text_ids,
    "cam3_attention_mask": cam3_attn_mask,
    
    "state": robot_state,
    "embodiment_id": embodiment_ids,
}

# Generate actions
with torch.no_grad():
    outputs = model.get_action(inputs)

actions = outputs["action"]  # [batch, action_horizon, action_dim]
routing_weights = outputs.get("routing_weights")  # [batch, 2]

print(f"Predicted Actions: {actions[0, 0]}")  # First timestep
if routing_weights is not None:
    print(f"Camera Weights - Left Wrist: {routing_weights[0, 0]:.3f}, Right Wrist: {routing_weights[0, 1]:.3f}")
```

---

## ì„¤ì • ì˜µì…˜

### Camera Router Config

```python
@dataclass
class Gr00tN1d6Config:
    # Camera MoE í™œì„±í™”
    use_camera_moe: bool = False
    
    # Router Architecture
    camera_router_hidden_dim: int = 512
    # Router MLP hidden dimension (256-1024 ê¶Œì¥)
    
    camera_router_temperature: float = 1.0
    # Softmax temperature (0.5-2.0)
    # - ë‚®ìŒ: ë” discreteí•œ ì„ íƒ
    # - ë†’ìŒ: ë” smoothí•œ blending
    
    # Gumbel-Softmax (ì„ íƒì )
    camera_router_use_gumbel: bool = False
    # Training ì¤‘ discrete samplingì„ ìœ„í•´ ì‚¬ìš©
    
    camera_router_gumbel_temp: float = 1.0
    # Gumbel-Softmax temperature
    
    # Loss Weight
    camera_routing_loss_weight: float = 0.1
    # Routing loss ê°€ì¤‘ì¹˜ (0.05-0.2 ê¶Œì¥)
    # - ë„ˆë¬´ ë‚®ìŒ: Routerê°€ í•™ìŠµë˜ì§€ ì•ŠìŒ
    # - ë„ˆë¬´ ë†’ìŒ: Action prediction ì„±ëŠ¥ ì €í•˜
    
    # Advanced Features
    camera_router_use_attention_pooling: bool = True
    # Promptì— ëŒ€í•œ learnable attention pooling (ê¶Œì¥)
    
    camera_router_use_learnable_scales: bool = True
    # Gated featuresì— ëŒ€í•œ learnable scaling (ê¶Œì¥)
```

### íŠœë‹ ê°€ì´ë“œ

| íŒŒë¼ë¯¸í„° | ê¶Œì¥ ë²”ìœ„ | ì„¤ëª… |
|---------|----------|------|
| `camera_router_hidden_dim` | 256-1024 | Router MLP í¬ê¸° |
| `camera_router_temperature` | 0.5-2.0 | ì‘ì„ìˆ˜ë¡ discrete, í´ìˆ˜ë¡ smooth |
| `camera_routing_loss_weight` | 0.05-0.2 | Routing loss ê°€ì¤‘ì¹˜ |
| `camera_router_use_attention_pooling` | True (ê¶Œì¥) | Prompt attention pooling |
| `camera_router_use_learnable_scales` | True (ê¶Œì¥) | Learnable magnitude compensation |

---

## ë°ì´í„° í˜•ì‹

### Multi-Camera Input Format

ê° ì¹´ë©”ë¼ë³„ë¡œ ë…ë¦½ì ì¸ ì…ë ¥ì„ ì œê³µ:

```python
# Camera prefix: cam1_, cam2_, cam3_
inputs = {
    # Camera 1 (Base - always required)
    "cam1_pixel_values": torch.Tensor,  # [batch, C, H, W]
    "cam1_input_ids": torch.LongTensor,  # [batch, seq_len]
    "cam1_attention_mask": torch.LongTensor,  # [batch, seq_len]
    
    # Camera 2 (Left Wrist - optional, will be zero-padded if missing)
    "cam2_pixel_values": torch.Tensor,
    "cam2_input_ids": torch.LongTensor,
    "cam2_attention_mask": torch.LongTensor,
    
    # Camera 3 (Right Wrist - optional, will be zero-padded if missing)
    "cam3_pixel_values": torch.Tensor,
    "cam3_input_ids": torch.LongTensor,
    "cam3_attention_mask": torch.LongTensor,
    
    # State and Action
    "state": torch.Tensor,  # [batch, state_dim]
    "action": torch.Tensor,  # [batch, action_horizon, action_dim]
    "embodiment_id": torch.LongTensor,  # [batch]
    "action_mask": torch.Tensor,  # [batch, action_horizon, action_dim]
    
    # Ground Truth Labels (for training only)
    "cam2_activate": torch.LongTensor,  # [batch] - binary (0 or 1)
    "cam3_activate": torch.LongTensor,  # [batch] - binary (0 or 1)
}
```

### Ground Truth Labels

Routing lossë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ê° ìƒ˜í”Œì— ëŒ€í•´ ì–´ë–¤ ì¹´ë©”ë¼ê°€ í™œì„±í™”ë˜ì–´ì•¼ í•˜ëŠ”ì§€ ë ˆì´ë¸” í•„ìš”:

```python
# ì˜ˆì‹œ: Left Wrist ì¹´ë©”ë¼ê°€ ì¤‘ìš”í•œ ê²½ìš°
cam2_activate = torch.tensor([1, 1, 0, 1])  # batch=4, cam2 active for samples 0,1,3
cam3_activate = torch.tensor([0, 0, 1, 0])  # cam3 active for sample 2
```

**ë ˆì´ë¸” ìƒì„± ê°€ì´ë“œ**:
- Taskë‚˜ object locationì— ë”°ë¼ ê²°ì •
- ì˜ˆ: "pick up object with left hand" â†’ cam2_activate=1
- ì˜ˆ: "manipulate with right hand" â†’ cam3_activate=1
- ë‘˜ ë‹¤ 0ì¸ ê²½ìš°: Routerê°€ ìë™ìœ¼ë¡œ ì„ íƒ (default cam2)

---

## ì˜ˆì œ

### Example 1: ê¸°ë³¸ ì‚¬ìš© (ë‹¨ì¼ ì¹´ë©”ë¼)

Camera MoEë¥¼ ë¹„í™œì„±í™”í•˜ê³  ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©:

```python
config = Gr00tN1d6Config(
    use_camera_moe=False,  # ë¹„í™œì„±í™”
    # ë‹¤ë¥¸ ì„¤ì •...
)

model = Gr00tN1d6(config)

# ê¸°ì¡´ ì…ë ¥ í˜•ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš©
inputs = {
    "pixel_values": images,
    "input_ids": text_ids,
    "attention_mask": attn_mask,
    "state": state,
    "action": action,
    "embodiment_id": embodiment_id,
}

outputs = model(inputs)
```

### Example 2: Multi-Camera Training

```python
config = Gr00tN1d6Config(
    use_camera_moe=True,
    camera_router_hidden_dim=512,
    camera_routing_loss_weight=0.1,
    camera_router_use_attention_pooling=True,
    camera_router_use_learnable_scales=True,
)

model = Gr00tN1d6(config)

# Training loop
for batch in dataloader:
    inputs = {
        "cam1_pixel_values": batch["cam1_images"],
        "cam1_input_ids": batch["cam1_text"],
        "cam1_attention_mask": batch["cam1_mask"],
        
        "cam2_pixel_values": batch["cam2_images"],
        "cam2_input_ids": batch["cam2_text"],
        "cam2_attention_mask": batch["cam2_mask"],
        
        "cam3_pixel_values": batch["cam3_images"],
        "cam3_input_ids": batch["cam3_text"],
        "cam3_attention_mask": batch["cam3_mask"],
        
        "state": batch["state"],
        "action": batch["action"],
        "embodiment_id": batch["embodiment_id"],
        "action_mask": batch["action_mask"],
        
        "cam2_activate": batch["cam2_label"],
        "cam3_activate": batch["cam3_label"],
    }
    
    outputs = model(inputs)
    loss = outputs["loss"]
    
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    
    # Log routing weights
    if "routing_weights" in outputs:
        routing_weights = outputs["routing_weights"]
        print(f"Routing: Cam2={routing_weights[:, 0].mean():.3f}, Cam3={routing_weights[:, 1].mean():.3f}")
```

### Example 3: Inference with Router Analysis

```python
model.eval()

with torch.no_grad():
    outputs = model.get_action(inputs)
    
    actions = outputs["action"]
    routing_weights = outputs["routing_weights"]
    
    # Analyze router decisions
    cam2_weight = routing_weights[:, 0]
    cam3_weight = routing_weights[:, 1]
    
    print(f"Left Wrist weight: {cam2_weight.mean():.3f} Â± {cam2_weight.std():.3f}")
    print(f"Right Wrist weight: {cam3_weight.mean():.3f} Â± {cam3_weight.std():.3f}")
    
    # Identify which camera is more important
    dominant_camera = torch.argmax(routing_weights, dim=-1)
    print(f"Dominant camera: {'Left Wrist' if dominant_camera[0] == 0 else 'Right Wrist'}")
```

---

## Backward Compatibility

Camera MoEë¥¼ ë¹„í™œì„±í™”í•˜ë©´ ê¸°ì¡´ ë‹¨ì¼ ì¹´ë©”ë¼ ëª¨ë“œë¡œ ì‘ë™:

```python
# Old code - still works
config = Gr00tN1d6Config(use_camera_moe=False)
model = Gr00tN1d6(config)

# Use single camera input as before
outputs = model(single_camera_inputs)
```

---

## Troubleshooting

### Issue 1: "Camera MoE enabled but no multi-camera inputs found"

**ì›ì¸**: Camera MoEê°€ í™œì„±í™”ë˜ì—ˆì§€ë§Œ ì…ë ¥ì— `cam1_*`, `cam2_*`, `cam3_*` prefixê°€ ì—†ìŒ

**í•´ê²°**:
1. ì…ë ¥ ë°ì´í„°ì— camera prefix ì¶”ê°€
2. ë˜ëŠ” `use_camera_moe=False`ë¡œ ì„¤ì •

### Issue 2: Routing lossê°€ ê°ì†Œí•˜ì§€ ì•ŠìŒ

**ì›ì¸**: 
- Ground truth labelsê°€ ë¶€ì •í™•
- `camera_routing_loss_weight`ê°€ ë„ˆë¬´ ì‘ìŒ

**í•´ê²°**:
1. `cam2_activate`, `cam3_activate` ë ˆì´ë¸” í™•ì¸
2. `camera_routing_loss_weight`ë¥¼ 0.1 â†’ 0.2ë¡œ ì¦ê°€

### Issue 3: Action prediction ì„±ëŠ¥ì´ ì €í•˜ë¨

**ì›ì¸**: Routing lossê°€ ë„ˆë¬´ ê°•í•¨

**í•´ê²°**:
- `camera_routing_loss_weight`ë¥¼ 0.1 â†’ 0.05ë¡œ ê°ì†Œ

---

## FAQ

### Q1: 2ê°œì˜ ì¹´ë©”ë¼ë§Œ ìˆëŠ” ê²½ìš°ëŠ”?

**A**: ì—†ëŠ” ì¹´ë©”ë¼ëŠ” ìë™ìœ¼ë¡œ zero-paddingë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, cam2ë§Œ ìˆê³  cam3ê°€ ì—†ìœ¼ë©´:

```python
inputs = {
    "cam1_pixel_values": cam1_images,
    "cam1_input_ids": cam1_text,
    "cam1_attention_mask": cam1_mask,
    
    "cam2_pixel_values": cam2_images,
    "cam2_input_ids": cam2_text,
    "cam2_attention_mask": cam2_mask,
    
    # cam3ëŠ” ìƒëµ - ìë™ìœ¼ë¡œ zero padding
}
```

### Q2: Ground truth labels ì—†ì´ training ê°€ëŠ¥í•œê°€ìš”?

**A**: ê°€ëŠ¥í•˜ì§€ë§Œ ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Labels ì—†ì´ëŠ” routing lossê°€ ê³„ì‚°ë˜ì§€ ì•Šê³ , routerê°€ í•™ìŠµë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹ :

1. Pseudo-labels ì‚¬ìš© (ì˜ˆ: heuristic ê¸°ë°˜)
2. Unsupervised routing (êµ¬í˜„ í•„ìš”)

### Q3: ì–´ë–¤ ì¹´ë©”ë¼ë¥¼ base(cam1)ë¡œ ì„ íƒí•´ì•¼ í•˜ë‚˜ìš”?

**A**: 
- **í•­ìƒ ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´ë©”ë¼**: ëª¨ë“  ë°ì´í„°ì— ì¡´ì¬
- **ë„“ì€ ì‹œì•¼ê°**: ì „ì²´ sceneì„ ë³¼ ìˆ˜ ìˆëŠ” ì¹´ë©”ë¼
- **ì•ˆì •ì ì¸ ìœ„ì¹˜**: ë¡œë´‡ ì›€ì§ì„ì— ì˜í–¥ë°›ì§€ ì•ŠëŠ” ê³ ì • ì¹´ë©”ë¼

GR00Tì˜ ê²½ìš° L515/Externalì´ ì´ ì¡°ê±´ì„ ë§Œì¡±í•©ë‹ˆë‹¤.

---

## References

- Original PI05 Camera MoE Documentation: `CAMERA_MOE_ARCHITECTURE.md`
- GR00T N1.6 Paper: [arXiv:2503.14734](https://arxiv.org/abs/2503.14734)
- Focal Loss: Lin et al., "Focal Loss for Dense Object Detection"

---

**Last Updated**: January 15, 2026  
**Author**: GR00T Team
